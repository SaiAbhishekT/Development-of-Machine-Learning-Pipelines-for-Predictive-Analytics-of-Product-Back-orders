{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, pipeline1 = joblib.load('pipeline-1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create an IsolationForest object\n",
    "clf = IsolationForest(n_estimators=100, contamination=0.05, random_state=43)\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict outliers/anomalies in the data\n",
    "y_pred_train = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector', SelectKBest(k=15))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier',\n",
       "                                  RandomForestClassifier(n_estimators=200))]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector', SelectKBest(k=15))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier',\n",
       "                                  RandomForestClassifier(n_estimators=200))]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline-1-fully-trained.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "\n",
    "joblib.dump([X_train, y_train, pipeline1], 'pipeline-1-fully-trained.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (**follow** the steps similar to Part I)\n",
    "* **If you have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sku</th>\n",
       "      <td>3394011</td>\n",
       "      <td>3339400</td>\n",
       "      <td>3400245</td>\n",
       "      <td>3473283</td>\n",
       "      <td>3318401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national_inv</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_time</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_transit_qty</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_3_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_6_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forecast_9_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_1_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_3_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_6_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sales_9_month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_bank</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potential_issue</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pieces_past_due</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local_bo_qty</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deck_risk</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oe_constraint</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppap_risk</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_stop</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went_on_backorder</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0        1        2        3        4\n",
       "sku                3394011  3339400  3400245  3473283  3318401\n",
       "national_inv          30.0      0.0    145.0     33.0     14.0\n",
       "lead_time              2.0      8.0      8.0      8.0      8.0\n",
       "in_transit_qty         0.0      0.0      0.0      0.0      1.0\n",
       "forecast_3_month       0.0      0.0      0.0      0.0     59.0\n",
       "forecast_6_month       0.0      0.0    102.0      0.0    101.0\n",
       "forecast_9_month       0.0      0.0    102.0     12.0    152.0\n",
       "sales_1_month          0.0      0.0     14.0      1.0     14.0\n",
       "sales_3_month          0.0      0.0     49.0      6.0     49.0\n",
       "sales_6_month          0.0      0.0    117.0     26.0     97.0\n",
       "sales_9_month          0.0      0.0    174.0     39.0    140.0\n",
       "min_bank               0.0      0.0     47.0      0.0     15.0\n",
       "potential_issue         No       No       No       No       No\n",
       "pieces_past_due        0.0      0.0      0.0      0.0      0.0\n",
       "perf_6_month_avg      0.99     0.99     0.84     0.99     0.79\n",
       "perf_12_month_avg     0.99     0.99     0.87     0.97     0.66\n",
       "local_bo_qty           0.0      0.0      0.0      0.0      0.0\n",
       "deck_risk               No       No       No       No       No\n",
       "oe_constraint           No       No       No       No       No\n",
       "ppap_risk               No       No       No       No       No\n",
       "stop_auto_buy          Yes      Yes      Yes      Yes      Yes\n",
       "rev_stop                No       No       No       No       No\n",
       "went_on_backorder       No       No       No       No       No"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "\n",
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET).sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "dataset.head().transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop('sku', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "# All the column names of these yes/no columns\n",
    "yes_no_columns = list(filter(lambda i: dataset[i].dtype!=np.float64, dataset.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Replacing NA values with mode\n",
    "for column_name in yes_no_columns:\n",
    "    mode = dataset[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    dataset[column_name].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 242076 entries, 0 to 242075\n",
      "Data columns (total 22 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   national_inv       242075 non-null  float64\n",
      " 1   lead_time          227351 non-null  float64\n",
      " 2   in_transit_qty     242075 non-null  float64\n",
      " 3   forecast_3_month   242075 non-null  float64\n",
      " 4   forecast_6_month   242075 non-null  float64\n",
      " 5   forecast_9_month   242075 non-null  float64\n",
      " 6   sales_1_month      242075 non-null  float64\n",
      " 7   sales_3_month      242075 non-null  float64\n",
      " 8   sales_6_month      242075 non-null  float64\n",
      " 9   sales_9_month      242075 non-null  float64\n",
      " 10  min_bank           242075 non-null  float64\n",
      " 11  potential_issue    242076 non-null  int64  \n",
      " 12  pieces_past_due    242075 non-null  float64\n",
      " 13  perf_6_month_avg   242075 non-null  float64\n",
      " 14  perf_12_month_avg  242075 non-null  float64\n",
      " 15  local_bo_qty       242075 non-null  float64\n",
      " 16  deck_risk          242076 non-null  int64  \n",
      " 17  oe_constraint      242076 non-null  int64  \n",
      " 18  ppap_risk          242076 non-null  int64  \n",
      " 19  stop_auto_buy      242076 non-null  int64  \n",
      " 20  rev_stop           242076 non-null  int64  \n",
      " 21  went_on_backorder  242076 non-null  int64  \n",
      "dtypes: float64(15), int64(7)\n",
      "memory usage: 40.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Convert yes/no columns into binary (0s and 1s)\n",
    "dataset = dataset.replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month', 'forecast_6_month', 'forecast_9_month', 'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank', 'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg', 'local_bo_qty']\n"
     ]
    }
   ],
   "source": [
    "# Check which columns have missing values\n",
    "missing_cols = dataset.isna().sum()[dataset.isna().sum() > 0].index.tolist()\n",
    "print(missing_cols)\n",
    "\n",
    "# Replace missing values with a specified value, e.g. 0\n",
    "dataset[missing_cols] = dataset[missing_cols].fillna(dataset.lead_time.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# Count number of NaN values\n",
    "num_nans = dataset.isna().sum().sum()\n",
    "print('Number of NaN values:', num_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a smart sample of the dataset\n",
    "\n",
    "y = dataset.went_on_backorder\n",
    "X = dataset.drop('went_on_backorder', axis=1)\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# create RandomUnderSampler object\n",
    "rus = RandomUnderSampler(random_state=43)\n",
    "\n",
    "# fit and resample data\n",
    "X_test, y_test = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2414  274]\n",
      " [ 580 2108]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85      2688\n",
      "           1       0.88      0.78      0.83      2688\n",
      "\n",
      "    accuracy                           0.84      5376\n",
      "   macro avg       0.85      0.84      0.84      5376\n",
      "weighted avg       0.85      0.84      0.84      5376\n",
      "\n",
      "Accuracy: 0.8411458333333334\n"
     ]
    }
   ],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = pipeline1.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Generate a classification report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    "The Pipeline 1 has an accuracy of 0.9955 on the training dataset and 0.8411 on the test dataset. The high accuracy on the training dataset suggests that the model was able to fit the training data well, but the lower performance on the unused test set may indicate overfitting.\n",
    "\n",
    "From the confusion matrix, we can see that the model has a high precision and recall for both classes, with slightly higher precision for class 1. This means that the model is good at correctly identifying both positive and negative instances. However, the lower f1-score suggests that there may be some imbalance between precision and recall, indicating that there may be some false positives and false negatives in the model's predictions.\n",
    "\n",
    "In conclusion, the Pipeline 1 appears to have a good overall performance with high precision and recall for both classes, but there is clearly room for improvement to reduce the false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    "I have been working on developing a system to predict and prevent back orders of parts in the supply chain, with the goal of saving the company money.\n",
    "\n",
    "After evaluating several machine learning pipelines, I recommend using Pipeline 1, which achieved an accuracy of 99.6% on the training dataset and 84.1% on the test dataset. This pipeline utilizes the Random Forest algorithm for classification and Select K Best for feature selection. Also, Isolation Forest algorithm was used for outlier detection. \n",
    "\n",
    "Based on the evaluation metrics, I am highly confident in the accuracy and performance of this model. It shows high precision, recall, and F1-score for both classes, indicating a good balance between the ability to correctly identify positive and negative instances. \n",
    "\n",
    "To operationalize this model, I recommend integrating it into the company's supply chain management system. The model should be trained on new data periodically to ensure it continues to provide accurate predictions. Additionally, the model should be continuously evaluated for performance, and adjustments should be made as needed.\n",
    "\n",
    "In summary, I believe that implementing this predictive model will help the company save money by preventing back orders and ensuring the efficient flow of parts in the supply chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
