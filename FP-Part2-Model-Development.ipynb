{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Model Development\n",
    "\n",
    "In this part, we develop three unique pipelines for predicting backorder. We use the smart sample from Part I to fit and evaluate these pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features=['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
    "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
    "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
    "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
    "       'perf_12_month_avg', 'local_bo_qty']\n",
    "binary_features=['deck_risk', 'oe_constraint',\n",
    "       'ppap_risk', 'stop_auto_buy', 'rev_stop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the smart sample here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reload your smart sampling from local file \n",
    "# ----------------------------------\n",
    "\n",
    "X_resampled, y_resampled = joblib.load('sample-data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize/standardize the data if required; otherwise ignore. You can perform this step inside the pipeline (if required). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = sampled_df.went_on_backorder\n",
    "#X = sampled_df.drop('went_on_backorder', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Pipeline\n",
    "\n",
    "In this section, we design an operationalized machine learning pipeline, which includes:\n",
    "\n",
    "* Anomaly detection\n",
    "* Dimensionality Reduction\n",
    "* Train a classification model\n",
    "\n",
    "\n",
    "We are free to use any of the models that we learned in the past or we can use new models. Here is a pool of methods: \n",
    "\n",
    "### Pool of Anomaly Detection Methods (Discussed in M4)\n",
    "1. IsolationForest\n",
    "2. EllipticEnvelope\n",
    "3. LocalOutlierFactor\n",
    "4. OneClassSVM\n",
    "5. SGDOneClassSVM\n",
    "\n",
    "### Pool of Feature Selection Methods (Discussed in M3)\n",
    "\n",
    "1. VarianceThreshold\n",
    "1. SelectKBest with any scoring method (e.g, chi, f_classif, mutual_info_classif)\n",
    "1. SelectKPercentile\n",
    "3. SelectFpr, SelectFdr, or  SelectFwe\n",
    "1. GenericUnivariateSelect\n",
    "2. PCA\n",
    "3. Factor Analysis\n",
    "4. Variance Threshold\n",
    "5. RFE\n",
    "7. SelectFromModel\n",
    "\n",
    "\n",
    "### Classification Methods (Discussed in M1-M2\n",
    "1. Decision Tree\n",
    "2. Random Forest\n",
    "3. Logistic Regression\n",
    "4. Naive Bayes\n",
    "5. Linear SVC\n",
    "6. SVC with kernels\n",
    "7. KNeighborsClassifier\n",
    "8. GradientBoostingClassifier\n",
    "9. XGBClassifier\n",
    "10. LGBM Classifier\n",
    "\n",
    "\n",
    "\n",
    "It is difficult to fit an anomaly detection method in the sklearn pipeline without writing custom codes. For simplicity, we avoid fitting an anomaly detection method within a pipeline. So we can create the workflow in two steps. \n",
    "* Step I: fit an outlier with the training set\n",
    "* Step II: define a pipeline using a feature selection and a classification method. Then cross-validate this pipeline using the training data without outliers. \n",
    "* Note: if your smart sample is somewhat imbalanced, you might want to change the scoring method in GridSearchCV (see the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n",
    "\n",
    "\n",
    "Once we fit the pipeline with gridsearch, we identify the best model and give an unbiased evaluation using the test set that we created in Part II. For unbiased evaluation we report confusion matrix, precision, recall, f1-score, accuracy, and other measures if you like. \n",
    "\n",
    "**Optional: Those who are interested in writing custom codes for adding an outlier detection method into the sklearn pipeline, please follow this discussion [thread](https://stackoverflow.com/questions/52346725/can-i-add-outlier-detection-and-removal-to-scikit-learn-pipeline).**\n",
    "\n",
    "\n",
    "**Note:** <span style='background:yellow'>We will be using Grid Search to find the optimal parameters of the pipelines.</span>\n",
    "\n",
    "You can add more notebook cells or import any Python modules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 1st pipeline \n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation\n",
    "  \n",
    "Add cells as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest -> Standard Scalar -> Select K Best -> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anamoly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E201)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create an IsolationForest object\n",
    "clf = IsolationForest(n_estimators=100, contamination=0.05, random_state=43)\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict outliers/anomalies in the data\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [20] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector', SelectKBest(k=5))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier', RandomForestClassifier())]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E202)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "numeric_features = ['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month', \n",
    "                    'forecast_6_month', 'forecast_9_month', 'sales_1_month', 'sales_3_month', \n",
    "                    'sales_6_month', 'sales_9_month', 'min_bank', 'pieces_past_due', \n",
    "                    'perf_6_month_avg', 'perf_12_month_avg', 'local_bo_qty']\n",
    "binary_features = ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', \n",
    "                   'rev_stop']\n",
    "\n",
    "\n",
    "# define the pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('bin', 'passthrough', binary_features),\n",
    "])\n",
    "\n",
    "feature_selection = Pipeline([\n",
    "    ('selector', SelectKBest(f_classif, k=5))\n",
    "])\n",
    "\n",
    "binary_classification = Pipeline([\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('binary_classification', binary_classification)\n",
    "])\n",
    "\n",
    "pipeline1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:115: UserWarning: Features [20] are constant.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_selection/_univariate_selection.py:116: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search: {'binary_classification__classifier__max_depth': None, 'binary_classification__classifier__min_samples_leaf': 1, 'binary_classification__classifier__min_samples_split': 2, 'binary_classification__classifier__n_estimators': 200, 'feature_selection__selector__k': 15}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'feature_selection__selector__k': [5, 10, 15],\n",
    "    'binary_classification__classifier__n_estimators': [50, 100, 200],\n",
    "    'binary_classification__classifier__max_depth': [10, 20, None],\n",
    "    'binary_classification__classifier__min_samples_split': [2, 5, 10],\n",
    "    'binary_classification__classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline1, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print('Best parameters found by grid search:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector', SelectKBest(k=15))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier',\n",
       "                                  RandomForestClassifier(n_estimators=200))]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given an unbiased evaluation  (Question #E203)\n",
    "# ----------------------------------\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "pipeline1.set_params(**best_params)\n",
    "pipeline1.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2793   15]\n",
      " [  10 2829]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      2808\n",
      "           1       0.99      1.00      1.00      2839\n",
      "\n",
      "    accuracy                           1.00      5647\n",
      "   macro avg       1.00      1.00      1.00      5647\n",
      "weighted avg       1.00      1.00      1.00      5647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = pipeline1.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Generate a classification report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.995572870550735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E204)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Best parameters found by grid search: {'binary_classification__classifier__max_depth': None, 'binary_classification__classifier__min_samples_leaf': 1, 'binary_classification__classifier__min_samples_split': 2, 'binary_classification__classifier__n_estimators': 200, 'feature_selection__selector__k': 15}\n",
    "\n",
    "Confusion Matrix:\n",
    " [[2793   15]\n",
    " [  10 2829]]\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.99      1.00      2808\n",
    "           1       0.99      1.00      1.00      2839\n",
    "\n",
    "    accuracy                           1.00      5647\n",
    "   macro avg       1.00      1.00      1.00      5647\n",
    "weighted avg       1.00      1.00      1.00      5647\n",
    "\n",
    "Accuracy: 0.995572870550735\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 2nd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EllipticEnvelope -> Standard Scalar -> PCA -> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:647: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\"The covariance matrix associated to your dataset \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [ 1 -1  1 ...  1  1  1]\n",
      "Number of inliers: 15245\n",
      "Number of outliers: 1694\n"
     ]
    }
   ],
   "source": [
    "# Add anomaly detection code  (Question #E205)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import numpy as np\n",
    "\n",
    "# Fit the elliptic envelope model\n",
    "clf = EllipticEnvelope(contamination=0.1)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict the labels for the data points\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Print the predicted labels and the number of inliers and outliers\n",
    "print(\"Predicted labels:\", y_pred_train)\n",
    "print(\"Number of inliers:\", len(y_pred_train[y_pred_train == 1]))\n",
    "print(\"Number of outliers:\", len(y_pred_train[y_pred_train == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('pca', PCA(n_components=5))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier', LogisticRegression())]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E206)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# define the pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('bin', 'passthrough', binary_features),\n",
    "])\n",
    "\n",
    "feature_selection = Pipeline([\n",
    "    ('pca', PCA(n_components=5))\n",
    "])\n",
    "\n",
    "binary_classification = Pipeline([\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('binary_classification', binary_classification)\n",
    "])\n",
    "\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55080044 0.55133169 0.54932404 0.55534557 0.60434384 0.60487561\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55877013 0.55457859 0.53716244 0.54430573 0.57854663 0.57701196\n",
      " 0.55398829 0.55286659 0.54483763 0.5496197  0.58704758 0.58964526\n",
      " 0.55398829 0.55286659 0.54483763 0.54973776 0.58710662 0.58964526\n",
      " 0.55369311 0.55274853 0.54477858 0.54973778 0.58710662 0.58958623\n",
      " 0.55505091 0.5532799  0.54105889 0.54507336 0.58297421 0.58025894\n",
      " 0.55894728 0.55475573 0.5373395  0.54489603 0.57860573 0.57831069\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55298462 0.55298462 0.5552866  0.5552866  0.62435608 0.62435608\n",
      " 0.55298462 0.55298462 0.55522757 0.5552866  0.62453319 0.62447416\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55469674 0.55304372 0.54200354 0.54401084 0.5840959  0.58008179\n",
      " 0.5577666  0.55440151 0.53804802 0.54483695 0.57919605 0.57831069\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55263043 0.55268946 0.55150855 0.55292531 0.62063704 0.62110929\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55800275 0.55434248 0.53751661 0.54471889 0.57913702 0.57836972\n",
      " 0.55286659 0.55274849 0.55021    0.55304337 0.61184118 0.61201835\n",
      " 0.55286659 0.55274849 0.55032807 0.55304337 0.61190025 0.61195931\n",
      " 0.55286659 0.55274849 0.55032807 0.55304337 0.61190023 0.61190027\n",
      " 0.55463769 0.55310277 0.54188544 0.54454214 0.58415493 0.57990469\n",
      " 0.55782565 0.55434248 0.53787089 0.54483693 0.57895992 0.57831069\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55298462 0.55298462 0.5552866  0.5552866  0.62435608 0.62435608\n",
      " 0.55298462 0.55298462 0.55522757 0.5552866  0.62453319 0.62447416\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55463771 0.55310277 0.54194448 0.54430599 0.5840959  0.58008179\n",
      " 0.55770757 0.55434248 0.53792992 0.54483693 0.57913702 0.57831069\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55292559 0.55292559 0.55522757 0.55540467 0.62447412 0.62465122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55782564 0.55428345 0.53787089 0.54465984 0.57919605 0.57831069\n",
      " 0.55286656 0.55298462 0.55428306 0.55522757 0.62234899 0.62288028\n",
      " 0.55286656 0.55298462 0.55428306 0.55522757 0.62228996 0.62288026\n",
      " 0.55286656 0.55298462 0.55434209 0.55522757 0.62234899 0.62282125\n",
      " 0.55463769 0.55310277 0.54194448 0.54460115 0.5840959  0.58008179\n",
      " 0.55782564 0.55434248 0.53792994 0.54483693 0.57913702 0.57825166\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55298462 0.55298462 0.5552866  0.5552866  0.62435608 0.62435608\n",
      " 0.55298462 0.55298462 0.55522757 0.5552866  0.62453319 0.62447416\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.55451964 0.55304372 0.5422397  0.54424696 0.5840959  0.58002276\n",
      " 0.55782564 0.55434248 0.53804802 0.54483693 0.57925508 0.57836972]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search: {'binary_classification__classifier__C': 10, 'binary_classification__classifier__penalty': 'l1', 'binary_classification__classifier__solver': 'liblinear', 'feature_selection__pca__n_components': 10, 'feature_selection__pca__whiten': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'binary_classification__classifier__C': [0.1, 1, 10],\n",
    "    'binary_classification__classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'binary_classification__classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'feature_selection__pca__n_components': [2, 5, 10],\n",
    "    'feature_selection__pca__whiten': [True, False]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline2, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print('Best parameters found by grid search:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('pca', PCA(n_components=10))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier',\n",
       "                                  LogisticRegression(C=10, penalty='l1',\n",
       "                                                     solver='liblinear'))]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given an unbiased evaluation  (Question #E207)\n",
    "# ----------------------------------\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "pipeline2.set_params(**best_params)\n",
    "pipeline2.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1426 1382]\n",
      " [ 419 2420]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.51      0.61      2808\n",
      "           1       0.64      0.85      0.73      2839\n",
      "\n",
      "    accuracy                           0.68      5647\n",
      "   macro avg       0.70      0.68      0.67      5647\n",
      "weighted avg       0.70      0.68      0.67      5647\n",
      "\n",
      "Accuracy: 0.6810695944749424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = pipeline2.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Generate a classification report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E208)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Best parameters found by grid search: {'binary_classification__classifier__C': 10, 'binary_classification__classifier__penalty': 'l1', 'binary_classification__classifier__solver': 'liblinear', 'feature_selection__pca__n_components': 10, 'feature_selection__pca__whiten': False}\n",
    "\n",
    "Confusion Matrix:\n",
    " [[1426 1382]\n",
    " [ 419 2420]]\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.77      0.51      0.61      2808\n",
    "           1       0.64      0.85      0.73      2839\n",
    "\n",
    "    accuracy                           0.68      5647\n",
    "   macro avg       0.70      0.68      0.67      5647\n",
    "weighted avg       0.70      0.68      0.67      5647\n",
    "\n",
    "Accuracy: 0.6810695944749424\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your 3rd pipeline\n",
    "  * Anomaly detection\n",
    "  * Dimensionality reduction\n",
    "  * Model training/validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IsolationForest -> Standard Scalar -> Variance Threshold -> Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly detection code  (Question #E209)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create an IsolationForest object\n",
    "clf = IsolationForest(n_estimators=100, contamination=0.05, random_state=43)\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict outliers/anomalies in the data\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector',\n",
       "                                  VarianceThreshold(threshold=0.01))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier', DecisionTreeClassifier())]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add codes for feature selection and classification pipeline with grid search  (Question #E210)\n",
    "# ----------------------------------\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "numeric_features = ['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month', \n",
    "                    'forecast_6_month', 'forecast_9_month', 'sales_1_month', 'sales_3_month', \n",
    "                    'sales_6_month', 'sales_9_month', 'min_bank', 'pieces_past_due', \n",
    "                    'perf_6_month_avg', 'perf_12_month_avg', 'local_bo_qty']\n",
    "binary_features = ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', \n",
    "                   'rev_stop']\n",
    "\n",
    "# define the pipeline\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('bin', 'passthrough', binary_features),\n",
    "])\n",
    "\n",
    "feature_selection = Pipeline([\n",
    "    ('selector', VarianceThreshold(threshold=0.01))\n",
    "])\n",
    "\n",
    "binary_classification = Pipeline([\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('binary_classification', binary_classification)\n",
    "])\n",
    "\n",
    "pipeline3.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by grid search: {'binary_classification__classifier__criterion': 'gini', 'binary_classification__classifier__max_depth': 10, 'binary_classification__classifier__min_samples_leaf': 1, 'binary_classification__classifier__min_samples_split': 2, 'feature_selection__selector__threshold': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "\n",
    "param_grid = {\n",
    "    'feature_selection__selector__threshold': [0.0, 0.01, 0.1, 1],\n",
    "    'binary_classification__classifier__criterion': ['gini', 'entropy'],\n",
    "    'binary_classification__classifier__max_depth': [None, 5, 10, 20],\n",
    "    'binary_classification__classifier__min_samples_split': [2, 5, 10],\n",
    "    'binary_classification__classifier__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline3, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print('Best parameters found by grid search:', grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['national_inv', 'lead_time',\n",
       "                                                   'in_transit_qty',\n",
       "                                                   'forecast_3_month',\n",
       "                                                   'forecast_6_month',\n",
       "                                                   'forecast_9_month',\n",
       "                                                   'sales_1_month',\n",
       "                                                   'sales_3_month',\n",
       "                                                   'sales_6_month',\n",
       "                                                   'sales_9_month', 'min_bank',\n",
       "                                                   'pieces_past_due',\n",
       "                                                   'perf_6_month_avg',\n",
       "                                                   'perf_12_month_avg',\n",
       "                                                   'local_bo_qty']),\n",
       "                                                 ('bin', 'passthrough',\n",
       "                                                  ['potential_issue',\n",
       "                                                   'deck_risk', 'oe_constraint',\n",
       "                                                   'ppap_risk', 'stop_auto_buy',\n",
       "                                                   'rev_stop'])])),\n",
       "                ('feature_selection',\n",
       "                 Pipeline(steps=[('selector',\n",
       "                                  VarianceThreshold(threshold=0.1))])),\n",
       "                ('binary_classification',\n",
       "                 Pipeline(steps=[('classifier',\n",
       "                                  DecisionTreeClassifier(max_depth=10))]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given an unbiased evaluation  (Question #E211)\n",
    "# ----------------------------------\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "pipeline3.set_params(**best_params)\n",
    "pipeline3.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2588  220]\n",
      " [ 131 2708]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94      2808\n",
      "           1       0.92      0.95      0.94      2839\n",
      "\n",
      "    accuracy                           0.94      5647\n",
      "   macro avg       0.94      0.94      0.94      5647\n",
      "weighted avg       0.94      0.94      0.94      5647\n",
      "\n",
      "Accuracy: 0.9378431025323181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = pipeline3.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Generate a classification report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the overall accuracy of the model\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Record the optimal hyperparameters and performance resulting from this pipeline.</center>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Detail Hyperparameters and Results below  (Question #E212)\n",
    "# ---------------------------------------------\n",
    "\n",
    "Best parameters found by grid search: {'binary_classification__classifier__criterion': 'gini', 'binary_classification__classifier__max_depth': 10, 'binary_classification__classifier__min_samples_leaf': 1, 'binary_classification__classifier__min_samples_split': 2, 'feature_selection__selector__threshold': 0.1}\n",
    "\n",
    "Confusion Matrix:\n",
    " [[2588  220]\n",
    " [ 131 2708]]\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.95      0.92      0.94      2808\n",
    "           1       0.92      0.95      0.94      2839\n",
    "\n",
    "    accuracy                           0.94      5647\n",
    "   macro avg       0.94      0.94      0.94      5647\n",
    "weighted avg       0.94      0.94      0.94      5647\n",
    "\n",
    "Accuracy: 0.9378431025323181\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare these three pipelines and discuss your findings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your analysis in this cell (Question #E213)\n",
    "# ----------------------------------\n",
    "\n",
    "Pipeline 1 achieved the highest accuracy of 99.6%. The precision and recall values for both classes are also very high, indicating a very low rate of false positives and false negatives. The F1 score is also 1.0 for both classes, which is a perfect score. This suggests that the model is highly accurate and reliable in predicting the target variable.\n",
    "\n",
    "Pipeline 2 achieved an accuracy of 68.1%, which is the lowest among the three models. Although the recall for class 1 is high (0.85), the precision is low (0.64), which suggests that there are many false positives in the predictions. The F1 score is also relatively low compared to Pipeline 1, indicating that the model's performance is not as good.\n",
    "\n",
    "Pipeline 3 achieved an accuracy of 93.8%, which is lower than Pipeline 1 but higher than Pipeline 2. The precision and recall values for both classes are high, with an F1 score of 0.94 for both classes, indicating a very accurate and reliable model\n",
    "\n",
    "Amongst the three, we select Pipeline 1 for further evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"background: yellow;\">Commit your code!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the required pipeline/models for Part III."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline-1.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump([X_resampled, y_resampled, pipeline1], 'pipeline-1.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have made a few commits so far of this project.  \n",
    "**Definitely make a commit of the notebook now!**  \n",
    "Comment should be: `Final Project, Checkpoint - Pipelines done`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
